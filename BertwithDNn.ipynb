{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KiranJana/Cs-Project/blob/main/BertwithDNn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abX1OgSkZsra",
        "outputId": "8ab8dc13-108b-4b31-e99c-90f1aaabc58a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Keras-Preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.22.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from Keras-Preprocessing) (1.16.0)\n",
            "Installing collected packages: Keras-Preprocessing\n",
            "Successfully installed Keras-Preprocessing-1.1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install Keras-Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPMu_JTZfh18"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SAcceusTkftB"
      },
      "outputs": [],
      "source": [
        "from natsort import natsorted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_CiCOJifsCz",
        "outputId": "e6eb326c-e0aa-43ff-d760-a95d53b228de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n",
            "/content/gdrive/MyDrive/Final Data/Final\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/', force_remount=True)\n",
        "%cd gdrive/MyDrive/Final Data/Final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykf5Dn8cgP1t",
        "outputId": "a4e7ab7f-af43-4b80-93f9-cba8bc18fb4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#importing necessary libraries\n",
        "import nltk \n",
        "from nltk import sent_tokenize # this helps to split text into Sentences\n",
        "from nltk import word_tokenize # this helps to split text into individual Words\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uPUUkINgXDC"
      },
      "outputs": [],
      "source": [
        "#function for preprocessing text\n",
        "def preprocess(text):\n",
        "    # Tokenize the text into sentences and words\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
        "\n",
        "    # Remove stop words and punctuations\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [[word for word in sentence if word.isalnum() and word not in stop_words] for sentence in words]\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in words]\n",
        "\n",
        "    # Combine the words back into sentences\n",
        "    sentences = [' '.join(sentence) for sentence in words]\n",
        "    return ' '.join(sentences)\n",
        "     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyIgfwfckAoS",
        "outputId": "b862216f-78d7-4902-fc52-24a87edc0862"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Collecting pycountry>=18.2.23\n",
            "  Downloading pycountry-22.3.5.tar.gz (10.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt<0.7,>=0.6.1\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.27.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.65.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2022.10.31)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pycountry>=18.2.23->sumy) (67.7.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.4)\n",
            "Building wheels for collected packages: breadability, docopt, pycountry\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21709 sha256=7a3dbb500ce306971def35307e076ac8d063db4da9107d520c88ad921ed81caf\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/22/90/b84fcc30e16598db20a0d41340616dbf9b1e82bbcc627b0b33\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=977d66d373584e3f1909d2e9c5e633e8ebdd6c4dbf467e23254717c9dfc5adc7\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for pycountry (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681847 sha256=0ead041b6078c759dd40cf16f2b92c69394a25122b2ccf98d47214fcaf3a8de2\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/57/cc/290c5252ec97a6d78d36479a3c5e5ecc76318afcb241ad9dbe\n",
            "Successfully built breadability docopt pycountry\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-22.3.5 sumy-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sumy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0m_vD-1geVt"
      },
      "outputs": [],
      "source": [
        "##importing packages necessary for summarization\n",
        "import sumy\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lex_rank import LexRankSummarizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK3kWdWsj0l_"
      },
      "source": [
        "**Getting resumes and Job descriptions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qcXuNXXjvzt"
      },
      "outputs": [],
      "source": [
        "def return_resume():\n",
        "    resumes = []\n",
        "    file_list = os.listdir()\n",
        "    sorted_files_list = natsorted(file_list)\n",
        "    for sorted_files in sorted_files_list:\n",
        "        for file in os.listdir():\n",
        "            if file == sorted_files:\n",
        "                files = open(file, 'r')\n",
        "                resume = summarize(files.read(), 40)\n",
        "                resumes.append(resume)\n",
        "    return resumes\n",
        "\n",
        "def get_job_description():\n",
        "    file_list = os.listdir()\n",
        "    for file in os.listdir():\n",
        "        if file.endswith('txt'):\n",
        "            job_desc = open(file, 'r')\n",
        "            job_desc_text = summarize(job_desc.read(), 40)\n",
        "    return job_desc_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTZv6FV2gkKe"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Function for cleaning data\n",
        "import re\n",
        "def clean_data(data):\n",
        "  text = re.sub(r\"\",\" \",data)\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'\\s+',\" \",text)\n",
        "  text = re.sub(r\",\",\" \",text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5Pasj1AgsvB"
      },
      "outputs": [],
      "source": [
        "#Function for text summarization\n",
        "def summarize(cleaned_article_content, num_of_sentence):\n",
        "  cleaned_article_content = clean_data(cleaned_article_content)\n",
        "  cleaned_article_content = preprocess(cleaned_article_content)\n",
        "  summarized_text =''\n",
        "  parser = PlaintextParser.from_string(cleaned_article_content,Tokenizer(\"english\"))\n",
        "  summarizer = LexRankSummarizer()\n",
        "  summary = summarizer(parser.document, num_of_sentence)\n",
        "  for sentence in summary:\n",
        "    summarized_text = summarized_text + str(sentence)\n",
        "    return summarized_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJdE0eXCiblr"
      },
      "source": [
        "**Chief Financial Officer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjUCkbE1hkTp",
        "outputId": "d65f94fb-3de0-40e6-993f-28957e095112"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total 40\n",
            "drwx------ 2 root root 4096 Apr 27 02:12 \u001b[0m\u001b[01;34mBusiness_Intelligence_Analyst\u001b[0m/\n",
            "drwx------ 2 root root 4096 Apr 27 02:12 \u001b[01;34mChief_Financial_Officer\u001b[0m/\n",
            "drwx------ 2 root root 4096 Apr 27 02:12 \u001b[01;34mFinancial_Controller\u001b[0m/\n",
            "drwx------ 2 root root 4096 Apr 27 02:12 \u001b[01;34mMarketing_Manager\u001b[0m/\n",
            "drwx------ 2 root root 4096 Apr 27 02:12 \u001b[01;34mOperation_Manager\u001b[0m/\n",
            "drwx------ 2 root root 4096 Apr 27 02:12 \u001b[01;34mSenior_Accountant\u001b[0m/\n",
            "drwx------ 2 root root 4096 Apr 27 02:12 \u001b[01;34mSenior_Cost_Accountant\u001b[0m/\n",
            "drwx------ 2 root root 4096 Apr 27 02:12 \u001b[01;34mVice_President_of_Finance\u001b[0m/\n",
            "drwx------ 2 root root 4096 Apr 27 02:12 \u001b[01;34mVP_Controller\u001b[0m/\n",
            "drwx------ 2 root root 4096 Apr 27 02:12 \u001b[01;34mVP_Marketing\u001b[0m/\n",
            "/content/gdrive/MyDrive/Final Data/Final/Chief_Financial_Officer/candidates\n"
          ]
        }
      ],
      "source": [
        "%ls -l\n",
        "%cd Chief_Financial_Officer/candidates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5dqVKWCNhpOU",
        "outputId": "23f7e834-b72e-498c-8dc9-97dacbeea2c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Final Data/Final/Chief_Financial_Officer\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "resumes = return_resume()\n",
        "targets = list(range(1, len(resumes)+1))\n",
        "percentages = []\n",
        "#we are getting resumes in ranked order so, giving rank. Rank 1 means highest\n",
        "for rank in targets:\n",
        "  percentage = targets[rank-1]\n",
        "  percentages.append(percentage)\n",
        "\n",
        "%cd ..\n",
        "job_desc = get_job_description()\n",
        "\n",
        "df_CFO = pd.DataFrame({'resume': resumes, 'job_description': [job_desc] * len(resumes), 'rank': percentages})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "id": "93JFd8Fpl2ox",
        "outputId": "e3cc383c-d863-467e-d1b0-46cba310a042"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2ec80b06-1a1c-498b-a5d2-03aec7823531\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>resume</th>\n",
              "      <th>job_description</th>\n",
              "      <th>rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c n e 8 b 0 7 8 0 0 b 8 3 7 4 f 2 e b 4 7 b f ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>c n e 2 6 6 9 f 0 1 1 f 1 0 4 1 8 5 3 f 0 3 8 ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>c n e 9 4 8 f c 9 6 1 9 e 4 6 2 7 b 1 8 9 5 8 ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>c n e e 3 5 5 f e 5 b 0 5 b 4 b 6 8 9 f 0 6 6 ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>c n e 8 9 f 0 c 0 6 6 1 3 0 e 4 5 0 b 5 6 1 5 ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>c n e f b 5 7 9 f c b c f 4 1 c 6 b b 5 b 4 f ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>c n e 9 b 0 e e b 9 8 f 0 4 2 5 3 b 0 0 b 1 1 ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>c n e f 7 8 c 2 5 0 3 1 9 7 8 4 9 3 f 8 1 f 8 ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>c n e 6 3 f 5 0 7 7 c 1 6 4 2 7 c c 3 8 8 b 6 ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>c n e c 2 3 e b 7 e c 6 4 0 0 f 6 5 9 f 8 f 0 ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>c n e 3 6 4 9 c 2 5 b b 8 7 f 4 6 4 6 b e 7 4 ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>c n e 8 9 3 c 4 4 0 f e 3 4 c 5 4 8 b 6 e 1 c ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>c n e b 4 7 e 5 5 b 5 e c 6 4 c 1 f b c 6 3 b ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>c n e 5 8 3 b 4 1 b 5 0 e 4 9 0 9 0 f 3 4 3 9 ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>c n e f 8 c 5 e f 1 3 1 1 1 4 5 1 8 2 7 e e 7 ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2ec80b06-1a1c-498b-a5d2-03aec7823531')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2ec80b06-1a1c-498b-a5d2-03aec7823531 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2ec80b06-1a1c-498b-a5d2-03aec7823531');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                               resume  \\\n",
              "0   c n e 8 b 0 7 8 0 0 b 8 3 7 4 f 2 e b 4 7 b f ...   \n",
              "1   c n e 2 6 6 9 f 0 1 1 f 1 0 4 1 8 5 3 f 0 3 8 ...   \n",
              "2   c n e 9 4 8 f c 9 6 1 9 e 4 6 2 7 b 1 8 9 5 8 ...   \n",
              "3   c n e e 3 5 5 f e 5 b 0 5 b 4 b 6 8 9 f 0 6 6 ...   \n",
              "4   c n e 8 9 f 0 c 0 6 6 1 3 0 e 4 5 0 b 5 6 1 5 ...   \n",
              "5   c n e f b 5 7 9 f c b c f 4 1 c 6 b b 5 b 4 f ...   \n",
              "6   c n e 9 b 0 e e b 9 8 f 0 4 2 5 3 b 0 0 b 1 1 ...   \n",
              "7   c n e f 7 8 c 2 5 0 3 1 9 7 8 4 9 3 f 8 1 f 8 ...   \n",
              "8   c n e 6 3 f 5 0 7 7 c 1 6 4 2 7 c c 3 8 8 b 6 ...   \n",
              "9   c n e c 2 3 e b 7 e c 6 4 0 0 f 6 5 9 f 8 f 0 ...   \n",
              "10  c n e 3 6 4 9 c 2 5 b b 8 7 f 4 6 4 6 b e 7 4 ...   \n",
              "11  c n e 8 9 3 c 4 4 0 f e 3 4 c 5 4 8 b 6 e 1 c ...   \n",
              "12  c n e b 4 7 e 5 5 b 5 e c 6 4 c 1 f b c 6 3 b ...   \n",
              "13  c n e 5 8 3 b 4 1 b 5 0 e 4 9 0 9 0 f 3 4 3 9 ...   \n",
              "14  c n e f 8 c 5 e f 1 3 1 1 1 4 5 1 8 2 7 e e 7 ...   \n",
              "\n",
              "                                      job_description  rank  \n",
              "0   j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...     1  \n",
              "1   j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...     2  \n",
              "2   j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...     3  \n",
              "3   j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...     4  \n",
              "4   j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...     5  \n",
              "5   j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...     6  \n",
              "6   j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...     7  \n",
              "7   j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...     8  \n",
              "8   j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...     9  \n",
              "9   j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...    10  \n",
              "10  j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...    11  \n",
              "11  j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...    12  \n",
              "12  j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...    13  \n",
              "13  j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...    14  \n",
              "14  j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...    15  "
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_CFO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PzcTlm7iqYK"
      },
      "source": [
        "**Business_Intelligence_Analyst**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "GomKNN1WiXms",
        "outputId": "2cb7ceef-5302-4403-ae16-48c7e09a1f38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Final Data/Final\n",
            "/content/gdrive/MyDrive/Final Data/Final/Business_Intelligence_Analyst/candidates\n",
            "/content/gdrive/MyDrive/Final Data/Final/Business_Intelligence_Analyst\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4acfb4db-50dc-420d-a2e0-181cd1f4f89e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>resume</th>\n",
              "      <th>job_description</th>\n",
              "      <th>rank</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c n e 4 f f 2 0 0 8 6 3 b 4 f 2 b f 7 5 9 b 3 ...</td>\n",
              "      <td>j b p n g 9 f e 0 3 4 3 2 1 c 8 4 5 1 1 b 5 7 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>c n e b 0 4 0 4 b 9 5 4 7 4 7 9 b 0 5 6 e 2 4 ...</td>\n",
              "      <td>j b p n g 9 f e 0 3 4 3 2 1 c 8 4 5 1 1 b 5 7 ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>c n e b b c 3 1 6 c 1 3 7 4 5 1 b 6 7 f 7 6 7 ...</td>\n",
              "      <td>j b p n g 9 f e 0 3 4 3 2 1 c 8 4 5 1 1 b 5 7 ...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4acfb4db-50dc-420d-a2e0-181cd1f4f89e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4acfb4db-50dc-420d-a2e0-181cd1f4f89e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4acfb4db-50dc-420d-a2e0-181cd1f4f89e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                              resume  \\\n",
              "0  c n e 4 f f 2 0 0 8 6 3 b 4 f 2 b f 7 5 9 b 3 ...   \n",
              "1  c n e b 0 4 0 4 b 9 5 4 7 4 7 9 b 0 5 6 e 2 4 ...   \n",
              "2  c n e b b c 3 1 6 c 1 3 7 4 5 1 b 6 7 f 7 6 7 ...   \n",
              "\n",
              "                                     job_description  rank  \n",
              "0  j b p n g 9 f e 0 3 4 3 2 1 c 8 4 5 1 1 b 5 7 ...     1  \n",
              "1  j b p n g 9 f e 0 3 4 3 2 1 c 8 4 5 1 1 b 5 7 ...     2  \n",
              "2  j b p n g 9 f e 0 3 4 3 2 1 c 8 4 5 1 1 b 5 7 ...     3  "
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%cd ..\n",
        "%cd Business_Intelligence_Analyst/candidates\n",
        "\n",
        "import pandas as pd\n",
        "resumes = return_resume()\n",
        "targets = list(range(1, len(resumes)+1))\n",
        "percentages = []\n",
        "\n",
        "for rank in targets:\n",
        "  percentage = targets[rank-1]\n",
        "  percentages.append(percentage)\n",
        "\n",
        "%cd ..\n",
        "job_desc = get_job_description()\n",
        "\n",
        "df_BI_Analyst = pd.DataFrame({'resume': resumes, 'job_description': [job_desc] * len(resumes), 'rank': percentages})\n",
        "df_BI_Analyst"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsSb8BS4iw2k"
      },
      "source": [
        "**Financial Controller**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W67RGygsi4-L",
        "outputId": "1f81f3a0-7899-4bdc-f606-7e7ba39089cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Final Data/Final\n",
            "/content/gdrive/MyDrive/Final Data/Final/Financial_Controller/candidates\n",
            "/content/gdrive/MyDrive/Final Data/Final/Financial_Controller\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "%cd Financial_Controller/candidates\n",
        "\n",
        "import pandas as pd\n",
        "resumes = return_resume()\n",
        "targets = list(range(1, len(resumes)+1))\n",
        "percentages = []\n",
        "\n",
        "for rank in targets:\n",
        "  percentage = targets[rank-1]\n",
        "  percentages.append(percentage)\n",
        "\n",
        "%cd ..\n",
        "job_desc = get_job_description()\n",
        "\n",
        "df_Financial_Controller = pd.DataFrame({'resume': resumes, 'job_description': [job_desc] * len(resumes), 'rank': percentages})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNX-W-sKi9Bz"
      },
      "source": [
        "**Marketing Manager**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ8UA2YijBhv",
        "outputId": "f1de8786-ffa2-4f32-c51a-1c24b23667e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Final Data/Final\n",
            "/content/gdrive/MyDrive/Final Data/Final/Marketing_Manager/candidates\n",
            "/content/gdrive/MyDrive/Final Data/Final/Marketing_Manager\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "%cd Marketing_Manager/candidates\n",
        "\n",
        "import pandas as pd\n",
        "resumes = return_resume()\n",
        "targets = list(range(1, len(resumes)+1))\n",
        "percentages = []\n",
        "\n",
        "for rank in targets:\n",
        "  percentage = targets[rank-1]\n",
        "  percentages.append(percentage)\n",
        "\n",
        "%cd ..\n",
        "job_desc = get_job_description()\n",
        "\n",
        "df_Marketing_Manager = pd.DataFrame({'resume': resumes, 'job_description': [job_desc] * len(resumes), 'rank': percentages})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWTLfMQ6jDK9"
      },
      "source": [
        "**Operation Manager**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6r528JFjK8O",
        "outputId": "bbb539d5-e3c9-4237-ca4a-136a3e731c0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Final Data/Final\n",
            "/content/gdrive/MyDrive/Final Data/Final/Operation_Manager/candidates\n",
            "/content/gdrive/MyDrive/Final Data/Final/Operation_Manager\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "%cd Operation_Manager/candidates\n",
        "\n",
        "import pandas as pd\n",
        "resumes = return_resume()\n",
        "targets = list(range(1, len(resumes)+1))\n",
        "percentages = []\n",
        "\n",
        "for rank in targets:\n",
        "  percentage = targets[rank-1]\n",
        "  percentages.append(percentage)\n",
        "\n",
        "%cd ..\n",
        "job_desc = get_job_description()\n",
        "\n",
        "df_Operation_Manager = pd.DataFrame({'resume': resumes, 'job_description': [job_desc] * len(resumes), 'rank': percentages})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fv0JXbrcjM1P"
      },
      "source": [
        "**Senior Accountant**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJFwzAgVjQg6",
        "outputId": "c2a8d689-1b0b-4a11-fffb-5f7ee50981d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Final Data/Final\n",
            "/content/gdrive/MyDrive/Final Data/Final/Senior_Accountant/candidates\n",
            "/content/gdrive/MyDrive/Final Data/Final/Senior_Accountant\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "%cd Senior_Accountant/candidates\n",
        "\n",
        "import pandas as pd\n",
        "resumes = return_resume()\n",
        "targets = list(range(1, len(resumes)+1))\n",
        "percentages = []\n",
        "for rank in targets:\n",
        "  percentage = targets[rank-1]\n",
        "  percentages.append(percentage)\n",
        "\n",
        "%cd ..\n",
        "job_desc = get_job_description()\n",
        "\n",
        "df_Senior_Accountant = pd.DataFrame({'resume': resumes, 'job_description': [job_desc] * len(resumes), 'rank': percentages})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyPuGPaqjSM2"
      },
      "source": [
        "**senior cost Accountant**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1y87VrMrjXH4",
        "outputId": "8df337df-6cda-4d2a-fa05-4befa674520e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Final Data/Final\n",
            "/content/gdrive/MyDrive/Final Data/Final/Senior_Cost_Accountant/candidates\n",
            "/content/gdrive/MyDrive/Final Data/Final/Senior_Cost_Accountant\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "%cd Senior_Cost_Accountant/candidates\n",
        "\n",
        "import pandas as pd\n",
        "resumes = return_resume()\n",
        "targets = list(range(1, len(resumes)+1))\n",
        "percentages = []\n",
        "\n",
        "for rank in targets:\n",
        "  percentage = targets[rank-1]\n",
        "  percentages.append(percentage)\n",
        "\n",
        "%cd ..\n",
        "job_desc = get_job_description()\n",
        "\n",
        "df_Senior_Cost_Accountant = pd.DataFrame({'resume': resumes, 'job_description': [job_desc] * len(resumes), 'rank': percentages})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwRKS4w-jScp"
      },
      "source": [
        "**Vice president of Finance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tqHs0ZnjfAq",
        "outputId": "966d082d-4fed-4ae1-ba01-67d0d7e79fea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Final Data/Final\n",
            "/content/gdrive/MyDrive/Final Data/Final/Vice_President_of_Finance/candidates\n",
            "/content/gdrive/MyDrive/Final Data/Final/Vice_President_of_Finance\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "%cd Vice_President_of_Finance/candidates\n",
        "\n",
        "import pandas as pd\n",
        "resumes = return_resume()\n",
        "targets = list(range(1, len(resumes)+1))\n",
        "percentages = []\n",
        "\n",
        "for rank in targets:\n",
        "  percentage = targets[rank-1]\n",
        "  percentages.append(percentage)\n",
        "\n",
        "%cd ..\n",
        "job_desc = get_job_description()\n",
        "\n",
        "df_Vice_President_of_Finance = pd.DataFrame({'resume': resumes, 'job_description': [job_desc] * len(resumes), 'rank': percentages})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbz_N3BGjhvS"
      },
      "source": [
        "**VP Controller**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezhvFuDujk1c",
        "outputId": "da54ab61-9378-42f7-c096-e64169f0dfb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Final Data/Final\n",
            "/content/gdrive/MyDrive/Final Data/Final/VP_Controller/candidates\n",
            "/content/gdrive/MyDrive/Final Data/Final/VP_Controller\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "%cd VP_Controller/candidates\n",
        "\n",
        "import pandas as pd\n",
        "resumes = return_resume()\n",
        "targets = list(range(1, len(resumes)+1))\n",
        "percentages = []\n",
        "\n",
        "for rank in targets:\n",
        "  percentage = targets[rank-1]\n",
        "  percentages.append(percentage)\n",
        "%cd ..\n",
        "job_desc = get_job_description()\n",
        "\n",
        "df_VP_Controller = pd.DataFrame({'resume': resumes, 'job_description': [job_desc] * len(resumes), 'rank': percentages})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQih5CJ7jmyq"
      },
      "source": [
        "**VP Marketing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4olwUMBjrUU",
        "outputId": "64c4246f-7976-4cd4-c330-a873c823111a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Final Data/Final\n",
            "/content/gdrive/MyDrive/Final Data/Final/VP_Marketing/candidates\n",
            "/content/gdrive/MyDrive/Final Data/Final/VP_Marketing\n"
          ]
        }
      ],
      "source": [
        "%cd ..\n",
        "%cd VP_Marketing/candidates\n",
        "\n",
        "import pandas as pd\n",
        "resumes = return_resume()\n",
        "targets = list(range(1, len(resumes)+1))\n",
        "percentages = []\n",
        "\n",
        "for rank in targets:\n",
        "  percentage = targets[rank-1]\n",
        "  percentages.append(percentage)\n",
        "\n",
        "%cd ..\n",
        "job_desc = get_job_description()\n",
        "\n",
        "df_VP_Marketing = pd.DataFrame({'resume': resumes, 'job_description': [job_desc] * len(resumes), 'rank': percentages})\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaNR5MRNjthj"
      },
      "source": [
        "**Merging all data Frames**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mi9UnbLHmaqU"
      },
      "outputs": [],
      "source": [
        "final_df = pd.concat([df_CFO, df_BI_Analyst, df_Financial_Controller, df_Marketing_Manager, df_Operation_Manager, df_Senior_Accountant, df_Senior_Cost_Accountant, df_Vice_President_of_Finance, df_VP_Controller, df_VP_Marketing], ignore_index = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npWaZ9fXm9dh"
      },
      "outputs": [],
      "source": [
        "label = final_df['rank']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiXLBEeEnNYR"
      },
      "outputs": [],
      "source": [
        "label1 = pd.get_dummies(label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAHA5kihndPf"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EhBm8z-nQen"
      },
      "outputs": [],
      "source": [
        "features = final_df[['resume','job_description']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "mrAzAOaooCFv",
        "outputId": "44e7a397-dc67-4b4c-c215-a58fb4df56a8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-20fef706-b184-4b38-bac9-c6fc71b403c5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>resume</th>\n",
              "      <th>job_description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>c n e 8 b 0 7 8 0 0 b 8 3 7 4 f 2 e b 4 7 b f ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>c n e 2 6 6 9 f 0 1 1 f 1 0 4 1 8 5 3 f 0 3 8 ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>c n e 9 4 8 f c 9 6 1 9 e 4 6 2 7 b 1 8 9 5 8 ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>c n e e 3 5 5 f e 5 b 0 5 b 4 b 6 8 9 f 0 6 6 ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>c n e 8 9 f 0 c 0 6 6 1 3 0 e 4 5 0 b 5 6 1 5 ...</td>\n",
              "      <td>j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>c n e f 6 1 4 f 9 b c 5 3 6 4 e 5 7 8 6 3 e f ...</td>\n",
              "      <td>j b p n g 6 6 f e 4 6 3 b b c f 4 c e 3 b c f ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>174</th>\n",
              "      <td>c n e 0 2 6 0 c 3 f 1 5 0 4 1 c e 9 7 5 4 7 3 ...</td>\n",
              "      <td>j b p n g 6 6 f e 4 6 3 b b c f 4 c e 3 b c f ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>175</th>\n",
              "      <td>c n e 2 4 c c e 3 6 0 9 9 4 e 4 b 8 7 4 7 1 3 ...</td>\n",
              "      <td>j b p n g 6 6 f e 4 6 3 b b c f 4 c e 3 b c f ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>176</th>\n",
              "      <td>c n e 2 5 3 f 3 1 8 3 b e 4 0 6 3 b 5 3 8 c e ...</td>\n",
              "      <td>j b p n g 6 6 f e 4 6 3 b b c f 4 c e 3 b c f ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>177</th>\n",
              "      <td>c n e 2 5 4 4 e 7 5 3 3 4 8 4 c 6 8 1 5 f b 6 ...</td>\n",
              "      <td>j b p n g 6 6 f e 4 6 3 b b c f 4 c e 3 b c f ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>178 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-20fef706-b184-4b38-bac9-c6fc71b403c5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-20fef706-b184-4b38-bac9-c6fc71b403c5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-20fef706-b184-4b38-bac9-c6fc71b403c5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                resume  \\\n",
              "0    c n e 8 b 0 7 8 0 0 b 8 3 7 4 f 2 e b 4 7 b f ...   \n",
              "1    c n e 2 6 6 9 f 0 1 1 f 1 0 4 1 8 5 3 f 0 3 8 ...   \n",
              "2    c n e 9 4 8 f c 9 6 1 9 e 4 6 2 7 b 1 8 9 5 8 ...   \n",
              "3    c n e e 3 5 5 f e 5 b 0 5 b 4 b 6 8 9 f 0 6 6 ...   \n",
              "4    c n e 8 9 f 0 c 0 6 6 1 3 0 e 4 5 0 b 5 6 1 5 ...   \n",
              "..                                                 ...   \n",
              "173  c n e f 6 1 4 f 9 b c 5 3 6 4 e 5 7 8 6 3 e f ...   \n",
              "174  c n e 0 2 6 0 c 3 f 1 5 0 4 1 c e 9 7 5 4 7 3 ...   \n",
              "175  c n e 2 4 c c e 3 6 0 9 9 4 e 4 b 8 7 4 7 1 3 ...   \n",
              "176  c n e 2 5 3 f 3 1 8 3 b e 4 0 6 3 b 5 3 8 c e ...   \n",
              "177  c n e 2 5 4 4 e 7 5 3 3 4 8 4 c 6 8 1 5 f b 6 ...   \n",
              "\n",
              "                                       job_description  \n",
              "0    j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...  \n",
              "1    j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...  \n",
              "2    j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...  \n",
              "3    j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...  \n",
              "4    j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...  \n",
              "..                                                 ...  \n",
              "173  j b p n g 6 6 f e 4 6 3 b b c f 4 c e 3 b c f ...  \n",
              "174  j b p n g 6 6 f e 4 6 3 b b c f 4 c e 3 b c f ...  \n",
              "175  j b p n g 6 6 f e 4 6 3 b b c f 4 c e 3 b c f ...  \n",
              "176  j b p n g 6 6 f e 4 6 3 b b c f 4 c e 3 b c f ...  \n",
              "177  j b p n g 6 6 f e 4 6 3 b b c f 4 c e 3 b c f ...  \n",
              "\n",
              "[178 rows x 2 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_dInaowoMxq",
        "outputId": "04f45b52-e2e8-4a0b-b6e5-072cdf4e950c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4QFKfFawWZj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e97bbfb8e938429bae416207099602ae",
            "b844e2d9c6ad453ea2f3b177a9fc6aad",
            "fb6c246e312d49199d84f4f8e6ba94bd",
            "68d2f00f87ed48c6b989ac2ae5f2a1cb",
            "40aa457c12c749fa9f4db641824a9780",
            "0b223e69ad244a0cb19526b88e144ba4",
            "0e0d7274ac06417d8aa01e2ae68975d7",
            "be7b299c8d114b7882a67428639cc68a",
            "eb4298185222444ba464fcde4925d9c8",
            "7f39a3641b114faf8e39e29818dd5cd2",
            "7bb506f8645c4e6597c0f010a276ac47",
            "f317c61cd80d477d9729231984d55d5b",
            "46ddf73442734d02abcb6f0cf8fc59c6",
            "deeb9f5a21af411a95b5fee308a2a4d5",
            "8361466cf85e43928179120851695da6",
            "aca3e4423647464ba4f07a0cefb5c21b",
            "360ffb3aa0fc416d81866b0e5e0b9f02",
            "e0e0043e257f46c492a8ee36ca18c090",
            "1747137991a14adf918a222647694c9b",
            "f0b9dbf3f64045c69bf0718f398af06d",
            "28e14eb15bfb470ab622b5acb5bba198",
            "4be59291d2984c59a9562e3808bb6bb9",
            "295f091ae133465e96eeefb523e74cc6",
            "b2f8cf394b2e4a2ca9717e2657f9283c",
            "4bae7c2da753471e8e4e82ff564ca315",
            "6b4fa3cde4c540e39a4a3ba0a4dbc989",
            "de38381046e644f6bf62c11ac2f8b709",
            "b62e6c94e1b1415394fd7c5c1c78c9a0",
            "5b3cb89ceee747d3a03b6f8a0f782051",
            "a78903f28bc444babc81e5699de7ec5f",
            "70eb5b371cbd4e54b784f8478661ee5a",
            "bdbfc893a70e45b49d09453bbbdaf0f1",
            "79c7647017ca45ed9f5ed0abe85c214a",
            "a1709558c25c44c2aebda9325c6b113c",
            "50506823603b415ba1b5321df2e8b6da",
            "0dec075d34fd42dbbaa9f9d4556c34ad",
            "989b0065cad54166acdec5e845e4ae02",
            "9bfb0cd583be4244b6f5f6b5b8a99525",
            "a35831de83a84e6fab07f7d068d2d053",
            "0a22f7401703472994a28d1d3a79a5dc",
            "7de3cd16da72490d9255e80939cab66b",
            "8bae0c6f4ace4915b03f29bb5d0704a1",
            "f44a533cedd9488698ce207fca91e8b9",
            "317df2dc270640ec998f0152a093cffa"
          ]
        },
        "id": "IslDVagLZxzj",
        "outputId": "34507a7e-1d4a-46d9-db6a-a7b4f639d387"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                resume  \\\n",
            "158  c n e 5 0 5 0 9 f 5 1 5 4 c 4 5 6 c e 8 4 f 7 ...   \n",
            "137  c n e 9 b 9 5 4 2 c 8 9 2 4 0 c 8 7 4 f 3 1 8 ...   \n",
            "98   c n e 9 c 1 7 f 9 4 4 4 1 4 8 8 8 8 f b 8 e e ...   \n",
            "159  c n e f 6 e 8 e 6 8 2 7 9 4 2 0 8 3 8 5 7 3 3 ...   \n",
            "38   c n e 8 2 9 5 4 7 b f 7 9 1 4 4 1 0 7 4 8 f 6 ...   \n",
            "..                                                 ...   \n",
            "71   c n e 6 7 0 0 6 9 5 b 1 9 5 9 4 6 4 5 2 9 e 0 ...   \n",
            "106  c n e 6 3 2 7 4 9 c 9 8 4 9 c 8 8 2 b e 0 8 c ...   \n",
            "14   c n e f 8 c 5 e f 1 3 1 1 1 4 5 1 8 2 7 e e 7 ...   \n",
            "92   c n e 3 9 5 e 6 2 4 0 5 2 4 e e b 2 8 8 c c 5 ...   \n",
            "102  c n e c 3 e 8 0 6 3 3 5 5 9 4 4 9 5 b 2 1 f e ...   \n",
            "\n",
            "                                       job_description  \n",
            "158  j b p n g 6 6 f e 4 6 3 b b c f 4 c e 3 b c f ...  \n",
            "137  j b p n g 1 b 3 6 f 0 b c f e 4 5 0 1 b e 0 e ...  \n",
            "98   j b p n g 7 0 1 e f 5 0 4 c 4 1 4 7 5 2 9 e 3 ...  \n",
            "159  j b p n g 6 6 f e 4 6 3 b b c f 4 c e 3 b c f ...  \n",
            "38   j b p n g c 8 9 b 3 9 2 f 4 3 4 6 e 8 b f 6 6 ...  \n",
            "..                                                 ...  \n",
            "71   j b p n g 3 6 0 9 9 1 8 0 7 1 b 2 4 6 2 f 8 c ...  \n",
            "106  j b p n g 7 0 1 e f 5 0 4 c 4 1 4 7 5 2 9 e 3 ...  \n",
            "14   j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...  \n",
            "92   j b p n g 5 0 4 5 9 8 9 9 3 f 2 e 4 8 8 7 c b ...  \n",
            "102  j b p n g 7 0 1 e f 5 0 4 c 4 1 4 7 5 2 9 e 3 ...  \n",
            "\n",
            "[142 rows x 2 columns]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e97bbfb8e938429bae416207099602ae",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f317c61cd80d477d9729231984d55d5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tf_model.h5:   0%|          | 0.00/536M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "295f091ae133465e96eeefb523e74cc6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1709558c25c44c2aebda9325c6b113c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "29/29 [==============================] - 2s 21ms/step - loss: 5.5058 - accuracy: 0.0282 - val_loss: 4.1491 - val_accuracy: 0.0556\n",
            "Epoch 2/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 4.0445 - accuracy: 0.0423 - val_loss: 3.5332 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/100\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 3.8264 - accuracy: 0.0493 - val_loss: 3.4407 - val_accuracy: 0.0278\n",
            "Epoch 4/100\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 3.5096 - accuracy: 0.0915 - val_loss: 3.2436 - val_accuracy: 0.0833\n",
            "Epoch 5/100\n",
            "29/29 [==============================] - 1s 19ms/step - loss: 3.6264 - accuracy: 0.0423 - val_loss: 3.3013 - val_accuracy: 0.0556\n",
            "Epoch 6/100\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 3.2823 - accuracy: 0.0845 - val_loss: 3.1423 - val_accuracy: 0.0278\n",
            "Epoch 7/100\n",
            "29/29 [==============================] - 1s 19ms/step - loss: 3.4185 - accuracy: 0.0352 - val_loss: 3.1281 - val_accuracy: 0.0278\n",
            "Epoch 8/100\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 3.4393 - accuracy: 0.0282 - val_loss: 3.1983 - val_accuracy: 0.0833\n",
            "Epoch 9/100\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 3.3489 - accuracy: 0.0563 - val_loss: 3.1892 - val_accuracy: 0.0278\n",
            "Epoch 10/100\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 3.4057 - accuracy: 0.0282 - val_loss: 3.1339 - val_accuracy: 0.0278\n",
            "Epoch 11/100\n",
            "29/29 [==============================] - 1s 18ms/step - loss: 3.3312 - accuracy: 0.0634 - val_loss: 3.1212 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/100\n",
            "29/29 [==============================] - 1s 17ms/step - loss: 3.2769 - accuracy: 0.0423 - val_loss: 3.1055 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/100\n",
            "29/29 [==============================] - 1s 19ms/step - loss: 3.2589 - accuracy: 0.0352 - val_loss: 3.1029 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/100\n",
            "29/29 [==============================] - 1s 19ms/step - loss: 3.1988 - accuracy: 0.0634 - val_loss: 3.1099 - val_accuracy: 0.0556\n",
            "Epoch 15/100\n",
            "29/29 [==============================] - 1s 17ms/step - loss: 3.2702 - accuracy: 0.0563 - val_loss: 3.1170 - val_accuracy: 0.0556\n",
            "Epoch 16/100\n",
            "29/29 [==============================] - 0s 15ms/step - loss: 3.1245 - accuracy: 0.1056 - val_loss: 3.1174 - val_accuracy: 0.0278\n",
            "Epoch 17/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.2135 - accuracy: 0.0352 - val_loss: 3.0830 - val_accuracy: 0.0833\n",
            "Epoch 18/100\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 3.2047 - accuracy: 0.0493 - val_loss: 3.0712 - val_accuracy: 0.0556\n",
            "Epoch 19/100\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 3.2225 - accuracy: 0.0704 - val_loss: 3.0972 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 3.2144 - accuracy: 0.0493 - val_loss: 3.0885 - val_accuracy: 0.0556\n",
            "Epoch 21/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.1743 - accuracy: 0.0845 - val_loss: 3.1000 - val_accuracy: 0.0278\n",
            "Epoch 22/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.1668 - accuracy: 0.0352 - val_loss: 3.0860 - val_accuracy: 0.0556\n",
            "Epoch 23/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.1268 - accuracy: 0.0493 - val_loss: 3.1054 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 3.1978 - accuracy: 0.0282 - val_loss: 3.1099 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/100\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 3.1343 - accuracy: 0.0563 - val_loss: 3.0931 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 3.1139 - accuracy: 0.0352 - val_loss: 3.1097 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.1218 - accuracy: 0.0493 - val_loss: 3.1007 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.1710 - accuracy: 0.0282 - val_loss: 3.0893 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.1518 - accuracy: 0.0493 - val_loss: 3.0965 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/100\n",
            "29/29 [==============================] - 0s 15ms/step - loss: 3.0880 - accuracy: 0.0704 - val_loss: 3.0859 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.0560 - accuracy: 0.0845 - val_loss: 3.1066 - val_accuracy: 0.0278\n",
            "Epoch 32/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 3.0812 - accuracy: 0.0704 - val_loss: 3.0921 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.0984 - accuracy: 0.0634 - val_loss: 3.1068 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.1004 - accuracy: 0.0845 - val_loss: 3.0958 - val_accuracy: 0.0556\n",
            "Epoch 35/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.0729 - accuracy: 0.0845 - val_loss: 3.0809 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/100\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 3.0460 - accuracy: 0.0704 - val_loss: 3.0834 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 3.0585 - accuracy: 0.0493 - val_loss: 3.0814 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.0677 - accuracy: 0.0493 - val_loss: 3.0800 - val_accuracy: 0.0278\n",
            "Epoch 39/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.0826 - accuracy: 0.0563 - val_loss: 3.1112 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.0589 - accuracy: 0.0423 - val_loss: 3.1077 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 3.0651 - accuracy: 0.0423 - val_loss: 3.0898 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 3.0312 - accuracy: 0.0634 - val_loss: 3.0977 - val_accuracy: 0.0278\n",
            "Epoch 43/100\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 3.0273 - accuracy: 0.0845 - val_loss: 3.0875 - val_accuracy: 0.0278\n",
            "Epoch 44/100\n",
            "29/29 [==============================] - 1s 17ms/step - loss: 3.0352 - accuracy: 0.0563 - val_loss: 3.0814 - val_accuracy: 0.0278\n",
            "Epoch 45/100\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 3.0034 - accuracy: 0.0352 - val_loss: 3.0833 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/100\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 3.0693 - accuracy: 0.0493 - val_loss: 3.0837 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/100\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 3.0376 - accuracy: 0.0493 - val_loss: 3.0814 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/100\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 3.0167 - accuracy: 0.0845 - val_loss: 3.0865 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/100\n",
            "29/29 [==============================] - 1s 18ms/step - loss: 3.0355 - accuracy: 0.0423 - val_loss: 3.0795 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/100\n",
            "29/29 [==============================] - 1s 17ms/step - loss: 3.0357 - accuracy: 0.0845 - val_loss: 3.0862 - val_accuracy: 0.0000e+00\n",
            "Epoch 51/100\n",
            "29/29 [==============================] - 1s 18ms/step - loss: 3.0147 - accuracy: 0.0845 - val_loss: 3.0870 - val_accuracy: 0.0000e+00\n",
            "Epoch 52/100\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 3.0284 - accuracy: 0.0563 - val_loss: 3.0839 - val_accuracy: 0.0000e+00\n",
            "Epoch 53/100\n",
            "29/29 [==============================] - 1s 18ms/step - loss: 2.9940 - accuracy: 0.0704 - val_loss: 3.0798 - val_accuracy: 0.0000e+00\n",
            "Epoch 54/100\n",
            "29/29 [==============================] - 1s 19ms/step - loss: 3.0324 - accuracy: 0.0634 - val_loss: 3.0858 - val_accuracy: 0.0000e+00\n",
            "Epoch 55/100\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 3.0217 - accuracy: 0.0775 - val_loss: 3.0882 - val_accuracy: 0.0000e+00\n",
            "Epoch 56/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 2.9949 - accuracy: 0.0775 - val_loss: 3.0940 - val_accuracy: 0.0000e+00\n",
            "Epoch 57/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.0065 - accuracy: 0.0634 - val_loss: 3.0923 - val_accuracy: 0.0000e+00\n",
            "Epoch 58/100\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 3.0181 - accuracy: 0.0634 - val_loss: 3.0854 - val_accuracy: 0.0000e+00\n",
            "Epoch 59/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.0191 - accuracy: 0.0211 - val_loss: 3.0843 - val_accuracy: 0.0000e+00\n",
            "Epoch 60/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 3.0134 - accuracy: 0.0493 - val_loss: 3.0840 - val_accuracy: 0.0000e+00\n",
            "Epoch 61/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.0001 - accuracy: 0.0704 - val_loss: 3.0849 - val_accuracy: 0.0000e+00\n",
            "Epoch 62/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.0193 - accuracy: 0.0704 - val_loss: 3.0838 - val_accuracy: 0.0000e+00\n",
            "Epoch 63/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 2.9988 - accuracy: 0.0634 - val_loss: 3.0917 - val_accuracy: 0.0000e+00\n",
            "Epoch 64/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.0371 - accuracy: 0.0352 - val_loss: 3.0868 - val_accuracy: 0.0000e+00\n",
            "Epoch 65/100\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 3.0191 - accuracy: 0.0634 - val_loss: 3.0814 - val_accuracy: 0.0000e+00\n",
            "Epoch 66/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 3.0205 - accuracy: 0.0493 - val_loss: 3.0802 - val_accuracy: 0.0000e+00\n",
            "Epoch 67/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.0070 - accuracy: 0.0282 - val_loss: 3.0775 - val_accuracy: 0.0000e+00\n",
            "Epoch 68/100\n",
            "29/29 [==============================] - 0s 15ms/step - loss: 3.0211 - accuracy: 0.0493 - val_loss: 3.0823 - val_accuracy: 0.0000e+00\n",
            "Epoch 69/100\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 3.0084 - accuracy: 0.0634 - val_loss: 3.0847 - val_accuracy: 0.0000e+00\n",
            "Epoch 70/100\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 3.0187 - accuracy: 0.0493 - val_loss: 3.0862 - val_accuracy: 0.0000e+00\n",
            "Epoch 71/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.0062 - accuracy: 0.0282 - val_loss: 3.0884 - val_accuracy: 0.0000e+00\n",
            "Epoch 72/100\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 3.0030 - accuracy: 0.0563 - val_loss: 3.0798 - val_accuracy: 0.0000e+00\n",
            "Epoch 73/100\n",
            "29/29 [==============================] - 0s 11ms/step - loss: 3.0162 - accuracy: 0.0493 - val_loss: 3.0932 - val_accuracy: 0.0000e+00\n",
            "Epoch 74/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 3.0114 - accuracy: 0.0563 - val_loss: 3.0863 - val_accuracy: 0.0000e+00\n",
            "Epoch 75/100\n",
            "29/29 [==============================] - 0s 14ms/step - loss: 3.0147 - accuracy: 0.0423 - val_loss: 3.0839 - val_accuracy: 0.0000e+00\n",
            "Epoch 76/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 3.0037 - accuracy: 0.0423 - val_loss: 3.0814 - val_accuracy: 0.0000e+00\n",
            "Epoch 77/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 2.9949 - accuracy: 0.0915 - val_loss: 3.0852 - val_accuracy: 0.0000e+00\n",
            "Epoch 78/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.0109 - accuracy: 0.0352 - val_loss: 3.0777 - val_accuracy: 0.0000e+00\n",
            "Epoch 79/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 3.0153 - accuracy: 0.0563 - val_loss: 3.0855 - val_accuracy: 0.0000e+00\n",
            "Epoch 80/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 3.0136 - accuracy: 0.0282 - val_loss: 3.0842 - val_accuracy: 0.0000e+00\n",
            "Epoch 81/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.0350 - accuracy: 0.0493 - val_loss: 3.0859 - val_accuracy: 0.0000e+00\n",
            "Epoch 82/100\n",
            "29/29 [==============================] - 0s 15ms/step - loss: 3.0175 - accuracy: 0.0704 - val_loss: 3.0878 - val_accuracy: 0.0000e+00\n",
            "Epoch 83/100\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 2.9991 - accuracy: 0.0563 - val_loss: 3.0850 - val_accuracy: 0.0000e+00\n",
            "Epoch 84/100\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 2.9959 - accuracy: 0.0704 - val_loss: 3.0783 - val_accuracy: 0.0000e+00\n",
            "Epoch 85/100\n",
            "29/29 [==============================] - 1s 17ms/step - loss: 2.9970 - accuracy: 0.0704 - val_loss: 3.0816 - val_accuracy: 0.0000e+00\n",
            "Epoch 86/100\n",
            "29/29 [==============================] - 1s 19ms/step - loss: 3.0042 - accuracy: 0.0493 - val_loss: 3.0859 - val_accuracy: 0.0000e+00\n",
            "Epoch 87/100\n",
            "29/29 [==============================] - 0s 16ms/step - loss: 2.9966 - accuracy: 0.0915 - val_loss: 3.0901 - val_accuracy: 0.0000e+00\n",
            "Epoch 88/100\n",
            "29/29 [==============================] - 1s 18ms/step - loss: 3.0133 - accuracy: 0.0423 - val_loss: 3.0903 - val_accuracy: 0.0000e+00\n",
            "Epoch 89/100\n",
            "29/29 [==============================] - 1s 17ms/step - loss: 2.9919 - accuracy: 0.0563 - val_loss: 3.0910 - val_accuracy: 0.0000e+00\n",
            "Epoch 90/100\n",
            "29/29 [==============================] - 1s 18ms/step - loss: 2.9920 - accuracy: 0.0915 - val_loss: 3.0911 - val_accuracy: 0.0000e+00\n",
            "Epoch 91/100\n",
            "29/29 [==============================] - 1s 18ms/step - loss: 3.0056 - accuracy: 0.0634 - val_loss: 3.0887 - val_accuracy: 0.0000e+00\n",
            "Epoch 92/100\n",
            "29/29 [==============================] - 1s 19ms/step - loss: 3.0065 - accuracy: 0.0634 - val_loss: 3.0886 - val_accuracy: 0.0000e+00\n",
            "Epoch 93/100\n",
            "29/29 [==============================] - 0s 17ms/step - loss: 3.0075 - accuracy: 0.0634 - val_loss: 3.0926 - val_accuracy: 0.0000e+00\n",
            "Epoch 94/100\n",
            "29/29 [==============================] - 1s 18ms/step - loss: 3.0385 - accuracy: 0.0352 - val_loss: 3.0872 - val_accuracy: 0.0000e+00\n",
            "Epoch 95/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 2.9968 - accuracy: 0.0563 - val_loss: 3.0830 - val_accuracy: 0.0000e+00\n",
            "Epoch 96/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.0119 - accuracy: 0.0704 - val_loss: 3.0848 - val_accuracy: 0.0000e+00\n",
            "Epoch 97/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.0306 - accuracy: 0.0634 - val_loss: 3.0956 - val_accuracy: 0.0000e+00\n",
            "Epoch 98/100\n",
            "29/29 [==============================] - 0s 12ms/step - loss: 3.0073 - accuracy: 0.0493 - val_loss: 3.0904 - val_accuracy: 0.0000e+00\n",
            "Epoch 99/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 3.0150 - accuracy: 0.0704 - val_loss: 3.0942 - val_accuracy: 0.0000e+00\n",
            "Epoch 100/100\n",
            "29/29 [==============================] - 0s 13ms/step - loss: 3.0183 - accuracy: 0.0352 - val_loss: 3.0860 - val_accuracy: 0.0000e+00\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 3.0860 - accuracy: 0.0000e+00\n",
            "Test Loss: 3.0860161781311035\n",
            "Test Accuracy: 0.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "\n",
        "# # combine all resumes into a single list\n",
        "# all_resumes = []\n",
        "# for job_desc in job_descs:\n",
        "#     all_resumes.extend(resumes[job_desc])\n",
        "\n",
        "# # create labels for each resume based on their job description\n",
        "# labels = []\n",
        "# for job_desc in job_descs:\n",
        "#     labels.extend([job_desc] * len(resumes[job_desc]))\n",
        "\n",
        "# # convert labels to integers\n",
        "# label_encoder = LabelEncoder()\n",
        "# labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features,label , test_size=0.2, random_state=42)\n",
        "# tokenize the text data\n",
        "\n",
        "print(X_train)\n",
        "\n",
        "# tokenizer = Tokenizer(num_words=512)\n",
        "# tokenizer.fit_on_texts(X_train)\n",
        "# tokenizer.fit_on_texts(X_test)\n",
        "# X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "# X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "\n",
        "# #pad sequences to ensure uniform length\n",
        "# max_len = 512\n",
        "# X_train_seq = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "# X_test_seq = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "# print(X_train_seq.shape)\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define the maximum sequence length for inputs\n",
        "max_length = 512\n",
        "X_train_seq = tokenizer(list(X_train['resume']), list(X_train['job_description']), padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
        "X_test_seq = tokenizer(list(X_test['resume']), list(X_test['job_description']), padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
        "\n",
        "\n",
        "# Get the BERT embeddings for the inputs\n",
        "\n",
        "X_train_seq = bert_model(X_train_seq)[1]\n",
        "#print(bert_model(X_train))\n",
        "X_test_seq = bert_model(X_test_seq)[1]\n",
        "\n",
        "\n",
        "\n",
        "# define the modela\n",
        "model = Sequential()\n",
        "model.add(Dense(512, input_dim=768))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, input_dim=768))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(final_df)))\n",
        "model.add(Activation('softmax'))\n",
        "optimizer = Adam(lr=0.00001)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# train the model\n",
        "batch_size = 5\n",
        "epochs = 100\n",
        "model.fit(X_train_seq, y_train, batch_size=batch_size, epochs=epochs, validation_data=(X_test_seq, y_test))\n",
        "\n",
        "# evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test_seq, y_test, batch_size=batch_size)\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dc9t2d-e1B17",
        "outputId": "bc6d7bbe-705a-4269-aa3f-ffa2a7198fae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                resume  \\\n",
            "158  c n e 5 0 5 0 9 f 5 1 5 4 c 4 5 6 c e 8 4 f 7 ...   \n",
            "137  c n e 9 b 9 5 4 2 c 8 9 2 4 0 c 8 7 4 f 3 1 8 ...   \n",
            "98   c n e 9 c 1 7 f 9 4 4 4 1 4 8 8 8 8 f b 8 e e ...   \n",
            "159  c n e f 6 e 8 e 6 8 2 7 9 4 2 0 8 3 8 5 7 3 3 ...   \n",
            "38   c n e 8 2 9 5 4 7 b f 7 9 1 4 4 1 0 7 4 8 f 6 ...   \n",
            "..                                                 ...   \n",
            "71   c n e 6 7 0 0 6 9 5 b 1 9 5 9 4 6 4 5 2 9 e 0 ...   \n",
            "106  c n e 6 3 2 7 4 9 c 9 8 4 9 c 8 8 2 b e 0 8 c ...   \n",
            "14   c n e f 8 c 5 e f 1 3 1 1 1 4 5 1 8 2 7 e e 7 ...   \n",
            "92   c n e 3 9 5 e 6 2 4 0 5 2 4 e e b 2 8 8 c c 5 ...   \n",
            "102  c n e c 3 e 8 0 6 3 3 5 5 9 4 4 9 5 b 2 1 f e ...   \n",
            "\n",
            "                                       job_description  \n",
            "158  j b p n g 6 6 f e 4 6 3 b b c f 4 c e 3 b c f ...  \n",
            "137  j b p n g 1 b 3 6 f 0 b c f e 4 5 0 1 b e 0 e ...  \n",
            "98   j b p n g 7 0 1 e f 5 0 4 c 4 1 4 7 5 2 9 e 3 ...  \n",
            "159  j b p n g 6 6 f e 4 6 3 b b c f 4 c e 3 b c f ...  \n",
            "38   j b p n g c 8 9 b 3 9 2 f 4 3 4 6 e 8 b f 6 6 ...  \n",
            "..                                                 ...  \n",
            "71   j b p n g 3 6 0 9 9 1 8 0 7 1 b 2 4 6 2 f 8 c ...  \n",
            "106  j b p n g 7 0 1 e f 5 0 4 c 4 1 4 7 5 2 9 e 3 ...  \n",
            "14   j b p n g 3 6 0 3 6 e 7 5 6 3 9 4 e c f b c 2 ...  \n",
            "92   j b p n g 5 0 4 5 9 8 9 9 3 f 2 e 4 8 8 7 c b ...  \n",
            "102  j b p n g 7 0 1 e f 5 0 4 c 4 1 4 7 5 2 9 e 3 ...  \n",
            "\n",
            "[142 rows x 2 columns]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
            "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-13b07e2bb75b>\u001b[0m in \u001b[0;36m<cell line: 40>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m#print(bert_model(X_train))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mX_test_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'input_tensor' is not defined"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "X_train, X_test, y_train1, y_test1 = train_test_split(features,label1 , test_size=0.2, random_state=42)\n",
        "# tokenize the text data\n",
        "\n",
        "print(X_train)\n",
        "\n",
        "# tokenizer = Tokenizer(num_words=512)\n",
        "# tokenizer.fit_on_texts(X_train)\n",
        "# tokenizer.fit_on_texts(X_test)\n",
        "# X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "# X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "\n",
        "# #pad sequences to ensure uniform length\n",
        "# max_len = 512\n",
        "# X_train_seq = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "# X_test_seq = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
        "# print(X_train_seq.shape)\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define the maximum sequence length for inputs\n",
        "max_length = 512\n",
        "X_train_seq = tokenizer(list(X_train['resume']), list(X_train['job_description']), padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
        "X_test_seq = tokenizer(list(X_test['resume']), list(X_test['job_description']), padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
        "\n",
        "\n",
        "# Get the BERT embeddings for the inputs\n",
        "\n",
        "X_train_seq = bert_model(X_train_seq)[1]\n",
        "#print(bert_model(X_train))\n",
        "X_test_seq = bert_model(X_test_seq)[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTqfktmdeuh3",
        "outputId": "0cef926a-a4f4-42db-e302-bdece50f672d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "142/142 [==============================] - 11s 41ms/step - loss: 3.3735 - accuracy: 0.0563 - val_loss: 2.9988 - val_accuracy: 0.0278\n",
            "Epoch 2/100\n",
            "142/142 [==============================] - 3s 25ms/step - loss: 3.0332 - accuracy: 0.0282 - val_loss: 2.9997 - val_accuracy: 0.0278\n",
            "Epoch 3/100\n",
            "142/142 [==============================] - 4s 26ms/step - loss: 3.0086 - accuracy: 0.0845 - val_loss: 3.0016 - val_accuracy: 0.0278\n",
            "Epoch 4/100\n",
            "142/142 [==============================] - 5s 37ms/step - loss: 3.0063 - accuracy: 0.0493 - val_loss: 3.0056 - val_accuracy: 0.0278\n",
            "Epoch 5/100\n",
            "142/142 [==============================] - 4s 25ms/step - loss: 3.0163 - accuracy: 0.0563 - val_loss: 3.0110 - val_accuracy: 0.0278\n",
            "Epoch 6/100\n",
            "142/142 [==============================] - 3s 24ms/step - loss: 3.0033 - accuracy: 0.0493 - val_loss: 3.0114 - val_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f362a7ead70>"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#input_tensor = tf.expand_dims(input_tensor, axis=-1) \n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Convolutional layer 1\n",
        "model.add(Conv1D(filters=32, kernel_size=5, activation='relu', input_shape=(768,1)))\n",
        "model.add(MaxPooling1D(3))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Convolutional layer 2\n",
        "model.add(Conv1D(filters=64, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(3))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "\n",
        "# Convolutional layer 3\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(3))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# # Convolutional layer 4\n",
        "# model.add(Conv1D(filters=256, kernel_size=5, activation='relu'))\n",
        "# model.add(MaxPooling1D(3))\n",
        "# model.add(Dropout(0.5))\n",
        "\n",
        "# # Convolutional layer 5\n",
        "# model.add(Conv1D(filters=512, kernel_size=5, activation='relu'))\n",
        "# model.add(MaxPooling1D(3))\n",
        "# model.add(Dropout(0.5))\n",
        "\n",
        "# Flattening layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Dense layer 1\n",
        "model.add(Dense(512, activation='relu', input_shape=(None, 128)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Dense layer 2\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Dense layer 3\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Dense layer 4\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "\n",
        "# Dense layer 5\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Output layer\n",
        "model.add(Dense(20, activation='softmax'))\n",
        "# Compile the model\n",
        "optimizer = Adam(lr=2e-5)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# train the model\n",
        "batch_size = 1\n",
        "epochs = 100\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "model.fit(X_train_seq, y_train1, batch_size=batch_size, epochs=epochs, validation_data=(X_test_seq, y_test1), callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuLf0plaz2Vj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEoq-2d4oJJb",
        "outputId": "f676239c-13c6-4338-8093-7d21611a613e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 10ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.00      0.00      0.00         3\n",
            "           2       0.00      0.00      0.00         3\n",
            "           3       0.00      0.00      0.00         2\n",
            "           4       0.00      0.00      0.00         2\n",
            "           5       0.06      1.00      0.11         2\n",
            "           7       0.00      0.00      0.00         2\n",
            "           8       0.00      0.00      0.00         3\n",
            "           9       0.00      0.00      0.00         1\n",
            "          10       0.00      0.00      0.00         2\n",
            "          11       0.00      0.00      0.00         1\n",
            "          12       0.00      0.00      0.00         3\n",
            "          13       0.00      0.00      0.00         4\n",
            "          14       0.00      0.00      0.00         3\n",
            "          16       0.00      0.00      0.00         1\n",
            "          17       0.00      0.00      0.00         2\n",
            "          18       0.00      0.00      0.00         1\n",
            "          20       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.06        36\n",
            "   macro avg       0.00      0.06      0.01        36\n",
            "weighted avg       0.00      0.06      0.01        36\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# make predictions on the test set\n",
        "y_pred = np.argmax(model.predict(X_test_seq), axis=-1)\n",
        "#print(y_pred)\n",
        "#print(y_test1)\n",
        "#print classification report\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# print confusion matrix\n",
        "#print(confusion_matrix(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlmnvgX-tJXf",
        "outputId": "b572412a-0483-428f-f896-3090f01a412b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 308ms/step\n",
            "Prediction for resume 1: I'm a software engineer with experience building web applications using Python and Django. \n",
            "Prediction for resume 2: I have a background in software engineering and data science, with experience building machine learning models. \n",
            "Prediction for resume 3: I'm a full-stack developer with experience building web and mobile applications using React and Node.js. \n"
          ]
        }
      ],
      "source": [
        "job_description = \"We're looking for a software engineer to join our team and help build the next generation of our product.\"\n",
        "resumes = [\"I'm a software engineer with experience building web applications using Python and Django.\",\n",
        "           \"I have a background in software engineering and data science, with experience building machine learning models.\",\n",
        "           \"I'm a full-stack developer with experience building web and mobile applications using React and Node.js.\"]\n",
        "\n",
        "# define the maximum sequence length for inputs\n",
        "max_length = 512\n",
        "\n",
        "# tokenize and pad the resumes and job description\n",
        "job_description_seq = tokenizer(job_description, padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
        "resumes_seq = tokenizer(resumes, padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
        "\n",
        "# get the BERT embeddings for the resumes and job description\n",
        "job_description_seq = bert_model(job_description_seq)[1]\n",
        "resumes_seq = bert_model(resumes_seq)[1]\n",
        "\n",
        "# make predictions on the resumes for the given job description\n",
        "predictions = model.predict([resumes_seq+job_description_seq])\n",
        "\n",
        "# print the predictions\n",
        "for i, prediction in enumerate(predictions):\n",
        "    print(f\"Prediction for resume {i+1}: {resumes[i]} \")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "623Ff4fJZ9Vs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgCdHsqraE_a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yu2PcFhAaLcS",
        "outputId": "e65a9f9c-e3cf-4034-9a6c-2d6d9b8a2df2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# import tensorflow as tf\n",
        "# from transformers import BertTokenizer, TFBertModel\n",
        "# from tensorflow.keras.layers import Input, Flatten, Dense, Dropout\n",
        "# from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "# # Load the pre-trained BERT model\n",
        "# bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# # Load the BERT tokenizer\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "# max_length = 512\n",
        "# X_train_job_seq = tokenizer(list(X_train['job_description']), padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
        "# X_train_resume_seq = tokenizer(list(X_train['resume']), padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
        "# X_test_job_seq = tokenizer(list(X_test['job_description']), padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
        "# X_test_resume_seq = tokenizer(list(X_test['resume']), padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')\n",
        "# # Get the BERT embeddings for the inputs\n",
        "# X_train_job_seq = bert_model(X_train_job_seq)[1]\n",
        "# X_train_resume_seq = bert_model(X_train_resume_seq)[1]\n",
        "# #print(bert_model(X_train))\n",
        "# X_test_job_seq = bert_model(X_test_job_seq)[1]\n",
        "# X_test_resume_seq = bert_model(X_test_resume_seq)[1]\n",
        "\n",
        "# # Define the maximum sequence length for inputs\n",
        "# max_seq_length = 512\n",
        "\n",
        "# # Define the input layers\n",
        "# job_input = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"job_input\")\n",
        "# resume_input = Input(shape=(max_seq_length,), dtype=tf.int32, name=\"resume_input\")\n",
        "\n",
        "# # Get the BERT embeddings for the inputs\n",
        "# job_output = bert_model(job_input)[1]\n",
        "# resume_output = bert_model(resume_input)[1]\n",
        "\n",
        "# # Concatenate the BERT embeddings\n",
        "# concatenated = tf.concat([job_output, resume_output], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z73wXdpisKiA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# # Add some dense layers\n",
        "# x = Flatten()(concatenated)\n",
        "# x = Dense(768, activation='relu')(x)\n",
        "# x = Dropout(0.2)(x)\n",
        "# x = Dense(256, activation='relu')(x)\n",
        "# x = Dropout(0.2)(x)\n",
        "# output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# # Define the model\n",
        "# model = Model(inputs=[job_input, resume_input], outputs=output)\n",
        "# # Compile the model\n",
        "# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# model.fit([X_train_job_seq,X_train_resume_seq], y_train, batch_size=batch_size, epochs=epochs, validation_data=([X_test_job_seq,X_test_resume_seq], y_test))\n",
        "\n",
        "# # Define a function to rank resumes based on job description\n",
        "# def rank_resumes(job_description, resumes):\n",
        "#     # Tokenize and encode the job description\n",
        "#     job_tokens = tokenizer.encode(job_description, add_special_tokens=True, max_length=max_seq_length, truncation=True, padding='max_length')\n",
        "#     job_input = np.array(job_tokens)[None, :]\n",
        "    \n",
        "#     # Tokenize and encode the resumes\n",
        "#     resume_inputs = []\n",
        "#     for resume in resumes:\n",
        "#         resume_tokens = tokenizer.encode(resume, add_special_tokens=True, max_length=max_seq_length, truncation=True, padding='max_length')\n",
        "#         resume_inputs.append(np.array(resume_tokens)[None, :])\n",
        "    \n",
        "#     # Make predictions for the resumes\n",
        "#     predictions = []\n",
        "#     for resume_input in resume_inputs:\n",
        "#         predictions.append(model.predict([job_input,resume_input])[0][0])\n",
        "    \n",
        "#     # Sort the predictions in descending order\n",
        "#     rankings = sorted(zip(predictions, resumes), reverse=True)\n",
        "    \n",
        "#     # Return the sorted list of rankings\n",
        "#     return rankings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRZnTTTA-v9A"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhJzm3rWaOzN",
        "outputId": "e8f159ea-5c8a-44a8-e95c-a9f97f972500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 12s 12s/step\n",
            "1/1 [==============================] - 4s 4s/step\n",
            "1/1 [==============================] - 6s 6s/step\n",
            "Rank 1: I'm a full-stack developer with experience building web and mobile applications using React and Node.js. (prediction score: 0.42534226179122925)\n",
            "Rank 2: I have a background in software engineering and data science, with experience building machine learning models. (prediction score: 0.4214506447315216)\n",
            "Rank 3: I'm a software engineer with experience building web applications using Python and Django. (prediction score: 0.4197404086589813)\n"
          ]
        }
      ],
      "source": [
        "job_description = \"We're looking for a software engineer to join our team and help build the next generation of our product.\"\n",
        "resumes = [\"I'm a software engineer with experience building web applications using Python and Django.\",\n",
        "           \"I have a background in software engineering and data science, with experience building machine learning models.\",\n",
        "           \"I'm a full-stack developer with experience building web and mobile applications using React and Node.js.\"]\n",
        "\n",
        "rankings = rank_resumes(job_description, resumes)\n",
        "\n",
        "for i, ranking in enumerate(rankings):\n",
        "    print(f\"Rank {i+1}: {ranking[1]} (prediction score: {ranking[0]})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tvqk3hAViP-9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw8JKrVIiLBN"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wbwFxEMJiRHw"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score, classification_report\n",
        "# from collections import defaultdict\n",
        "# from sklearn.preprocessing import MultiLabelBinarizer\n",
        "# from collections import defaultdict\n",
        "# # Define vectorizer\n",
        "# vectorizer = TfidfVectorizer()\n",
        "\n",
        "# # Transform text data into numerical vectors\n",
        "# X_train_vec = vectorizer.fit_transform(X_train)\n",
        "# X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# # Define logistic regression model\n",
        "# clf = LogisticRegression()\n",
        "# print(X_train.shape)\n",
        "# print(y_train.shape)\n",
        "# # Train model on training data\n",
        "# clf.fit(X_train_vec, y_train)\n",
        "\n",
        "# # Evaluate model on testing data\n",
        "# y_pred = clf.predict(X_test_vec)\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# report = classification_report(y_test, y_pred)\n",
        "\n",
        "# # Print evaluation metrics\n",
        "# print(f'Accuracy: {accuracy}')\n",
        "# print(f'Classification report: \\n{report}')\n",
        "\n",
        "# # Rank resumes based on job descriptions\n",
        "# for job_desc in job_descs:\n",
        "#     # Extract resumes for job description\n",
        "#     job_resumes = merged_df.loc[merged_df['job_desc'] == job_desc]['resume']\n",
        "    \n",
        "#     # Transform text data into numerical vectors\n",
        "#     job_resumes_vec = vectorizer.transform(job_resumes)\n",
        "    \n",
        "#     # Predict scores for resumes\n",
        "#     scores = clf.predict(job_resumes_vec)\n",
        "    \n",
        "#     # Combine resumes with their scores\n",
        "#     resume_scores = list(zip(job_resumes, scores))\n",
        "    \n",
        "#     # Sort resumes by score in descending order\n",
        "#     sorted_resumes = sorted(resume_scores, key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "#     #Print ranked list of resumes\n",
        "#     print(f'Job description: {job_desc}')\n",
        "#     for i, (resume, score) in enumerate(sorted_resumes):\n",
        "#         print(f'{i+1}. {resume} - score: {score}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9s8S7wii6fed"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPkmyTmqIQnh6Y7tgpRy4Yi",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a22f7401703472994a28d1d3a79a5dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0b223e69ad244a0cb19526b88e144ba4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dec075d34fd42dbbaa9f9d4556c34ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7de3cd16da72490d9255e80939cab66b",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8bae0c6f4ace4915b03f29bb5d0704a1",
            "value": 28
          }
        },
        "0e0d7274ac06417d8aa01e2ae68975d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1747137991a14adf918a222647694c9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "28e14eb15bfb470ab622b5acb5bba198": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "295f091ae133465e96eeefb523e74cc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2f8cf394b2e4a2ca9717e2657f9283c",
              "IPY_MODEL_4bae7c2da753471e8e4e82ff564ca315",
              "IPY_MODEL_6b4fa3cde4c540e39a4a3ba0a4dbc989"
            ],
            "layout": "IPY_MODEL_de38381046e644f6bf62c11ac2f8b709"
          }
        },
        "317df2dc270640ec998f0152a093cffa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "360ffb3aa0fc416d81866b0e5e0b9f02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40aa457c12c749fa9f4db641824a9780": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46ddf73442734d02abcb6f0cf8fc59c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_360ffb3aa0fc416d81866b0e5e0b9f02",
            "placeholder": "​",
            "style": "IPY_MODEL_e0e0043e257f46c492a8ee36ca18c090",
            "value": "Downloading tf_model.h5: 100%"
          }
        },
        "4bae7c2da753471e8e4e82ff564ca315": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a78903f28bc444babc81e5699de7ec5f",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70eb5b371cbd4e54b784f8478661ee5a",
            "value": 231508
          }
        },
        "4be59291d2984c59a9562e3808bb6bb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "50506823603b415ba1b5321df2e8b6da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a35831de83a84e6fab07f7d068d2d053",
            "placeholder": "​",
            "style": "IPY_MODEL_0a22f7401703472994a28d1d3a79a5dc",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "5b3cb89ceee747d3a03b6f8a0f782051": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68d2f00f87ed48c6b989ac2ae5f2a1cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f39a3641b114faf8e39e29818dd5cd2",
            "placeholder": "​",
            "style": "IPY_MODEL_7bb506f8645c4e6597c0f010a276ac47",
            "value": " 570/570 [00:00&lt;00:00, 14.3kB/s]"
          }
        },
        "6b4fa3cde4c540e39a4a3ba0a4dbc989": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bdbfc893a70e45b49d09453bbbdaf0f1",
            "placeholder": "​",
            "style": "IPY_MODEL_79c7647017ca45ed9f5ed0abe85c214a",
            "value": " 232k/232k [00:00&lt;00:00, 2.98MB/s]"
          }
        },
        "70eb5b371cbd4e54b784f8478661ee5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79c7647017ca45ed9f5ed0abe85c214a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7bb506f8645c4e6597c0f010a276ac47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7de3cd16da72490d9255e80939cab66b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f39a3641b114faf8e39e29818dd5cd2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8361466cf85e43928179120851695da6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28e14eb15bfb470ab622b5acb5bba198",
            "placeholder": "​",
            "style": "IPY_MODEL_4be59291d2984c59a9562e3808bb6bb9",
            "value": " 536M/536M [00:03&lt;00:00, 139MB/s]"
          }
        },
        "8bae0c6f4ace4915b03f29bb5d0704a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "989b0065cad54166acdec5e845e4ae02": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f44a533cedd9488698ce207fca91e8b9",
            "placeholder": "​",
            "style": "IPY_MODEL_317df2dc270640ec998f0152a093cffa",
            "value": " 28.0/28.0 [00:00&lt;00:00, 426B/s]"
          }
        },
        "9bfb0cd583be4244b6f5f6b5b8a99525": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1709558c25c44c2aebda9325c6b113c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50506823603b415ba1b5321df2e8b6da",
              "IPY_MODEL_0dec075d34fd42dbbaa9f9d4556c34ad",
              "IPY_MODEL_989b0065cad54166acdec5e845e4ae02"
            ],
            "layout": "IPY_MODEL_9bfb0cd583be4244b6f5f6b5b8a99525"
          }
        },
        "a35831de83a84e6fab07f7d068d2d053": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a78903f28bc444babc81e5699de7ec5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aca3e4423647464ba4f07a0cefb5c21b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2f8cf394b2e4a2ca9717e2657f9283c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b62e6c94e1b1415394fd7c5c1c78c9a0",
            "placeholder": "​",
            "style": "IPY_MODEL_5b3cb89ceee747d3a03b6f8a0f782051",
            "value": "Downloading (…)solve/main/vocab.txt: 100%"
          }
        },
        "b62e6c94e1b1415394fd7c5c1c78c9a0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b844e2d9c6ad453ea2f3b177a9fc6aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b223e69ad244a0cb19526b88e144ba4",
            "placeholder": "​",
            "style": "IPY_MODEL_0e0d7274ac06417d8aa01e2ae68975d7",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "bdbfc893a70e45b49d09453bbbdaf0f1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be7b299c8d114b7882a67428639cc68a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de38381046e644f6bf62c11ac2f8b709": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deeb9f5a21af411a95b5fee308a2a4d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1747137991a14adf918a222647694c9b",
            "max": 536063208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f0b9dbf3f64045c69bf0718f398af06d",
            "value": 536063208
          }
        },
        "e0e0043e257f46c492a8ee36ca18c090": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e97bbfb8e938429bae416207099602ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b844e2d9c6ad453ea2f3b177a9fc6aad",
              "IPY_MODEL_fb6c246e312d49199d84f4f8e6ba94bd",
              "IPY_MODEL_68d2f00f87ed48c6b989ac2ae5f2a1cb"
            ],
            "layout": "IPY_MODEL_40aa457c12c749fa9f4db641824a9780"
          }
        },
        "eb4298185222444ba464fcde4925d9c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f0b9dbf3f64045c69bf0718f398af06d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f317c61cd80d477d9729231984d55d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46ddf73442734d02abcb6f0cf8fc59c6",
              "IPY_MODEL_deeb9f5a21af411a95b5fee308a2a4d5",
              "IPY_MODEL_8361466cf85e43928179120851695da6"
            ],
            "layout": "IPY_MODEL_aca3e4423647464ba4f07a0cefb5c21b"
          }
        },
        "f44a533cedd9488698ce207fca91e8b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb6c246e312d49199d84f4f8e6ba94bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be7b299c8d114b7882a67428639cc68a",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb4298185222444ba464fcde4925d9c8",
            "value": 570
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}